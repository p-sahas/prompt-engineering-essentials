{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da7f40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.prompts import render\n",
    "from utils.llm_client import LLMClient # Pick llm client\n",
    "from utils.logging_utils import log_llm_call # logging every API call\n",
    "from utils.router import pick_model, should_use_reasoning_model # # general, resooning or strong\n",
    "from IPython.display import Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288c6ac",
   "metadata": {},
   "source": [
    "## 01 - Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec86d395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The sentiment is: positive',\n",
       " 'usage': {'input_tokens_est': 73,\n",
       "  'context_tokens_est': 0,\n",
       "  'total_est': 76,\n",
       "  'prompt_tokens_actual': 76,\n",
       "  'completion_tokens_actual': 5,\n",
       "  'total_tokens_actual': 81},\n",
       " 'latency_ms': 923,\n",
       " 'raw': ChatCompletion(id='chatcmpl-Cs62MWy4tWFwRYoBVG4hF1E0LOWOJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sentiment is: positive', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1767008954, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_8bbc38b4db', usage=CompletionUsage(completion_tokens=5, prompt_tokens=76, total_tokens=81, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))),\n",
       " 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text, spec = render(\n",
    "            \"zero_shot.v1\",\n",
    "            role=\"sentiment_analyst\",\n",
    "            instruction=\"Analyze the following text and determine the sentiment as positive / negative neutral.\",\n",
    "            constraints=\"The sentiment should be one of the following: positive, negative, neutral\",\n",
    "            format=\"The sentiment is: {sentiment}\"\n",
    "            )\n",
    "\n",
    "model = pick_model('openai', 'general')\n",
    "llm = LLMClient('openai', model)\n",
    "\n",
    "text = \"I'am really happy with the product! It's amazing!\"\n",
    "messages= [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{prompt_text}\\n\\nReview: {text}\"\n",
    "    }]\n",
    "\n",
    "llm.chat(messages, temperature=0.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467d8f5",
   "metadata": {},
   "source": [
    "## 02 - Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc12b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: negative\n"
     ]
    }
   ],
   "source": [
    "examples = \"\"\"\n",
    "Example 01:\n",
    "Review: I'm really happy with the product! It's amazing!\n",
    "Sentiment: positive\n",
    "Explanation: User says product is good also he is happy.\n",
    "\n",
    "Example 02:\n",
    "Review: I'm really unhappy with the product! It's bad!\n",
    "Sentiment: negative\n",
    "Explanation: User says product is bad and he is unhappy.\n",
    "\n",
    "Example 03:\n",
    "Review: The product is okay, but it's not great.\n",
    "Sentiment: neutral\n",
    "Explanation: User says product is okay but not great.\n",
    "\n",
    "Example 04:\n",
    "Review: I'm not sure about the product.\n",
    "Sentiment: neutral\n",
    "Explanation: User not sure about the product.\n",
    "\n",
    "Example 05:\n",
    "Review: I'm really happy with the product but It's bad!\n",
    "Sentiment: negative\n",
    "Explanation: User says product is good but he is happy. So we are prioratizing his word on product quality not his happiness.\n",
    "             That's why we are considering thee product quality as negative.\n",
    "\n",
    "Example 06:\n",
    "Review: I'm really unhappy with the product but It's amazing!\n",
    "Sentiment: positive\n",
    "Explanation: User says product is bad but he is happy. So we are prioratize his word on product quality not on his happiness.\n",
    "             That's why we are considering thee product quality as positive.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_text, spec = render(\n",
    "            \"zero_shot.v1\",\n",
    "            role=\"sentiment_analyst\",\n",
    "            examples=examples,\n",
    "            instruction=\"Analyze the following text and determine the sentiment as positive / negative neutral.\",\n",
    "            constraints=\"The sentiment should be one of the following: positive, negative, neutral\",\n",
    "            format=\"The sentiment is: {sentiment}\"\n",
    "            )\n",
    "\n",
    "model = pick_model('groq', 'reason')\n",
    "llm = LLMClient('groq', model)\n",
    "\n",
    "text = \"It seems nice but It's.\"\n",
    "messages= [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{prompt_text}\\n\\nReview: {text}\"\n",
    "    }]\n",
    "\n",
    "response = llm.chat(messages, temperature=0.4)\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a9d34",
   "metadata": {},
   "source": [
    "## 03 -  COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0c66a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Reasoning steps**\n",
       "\n",
       "1. **Average speed** is defined as total distance divided by total time (including any stops).  \n",
       "2. Without any stop:  \n",
       "   - Distance = 100 mi  \n",
       "   - Time = 2 h  \n",
       "   - Speed = 100 mi / 2 h = 50 mph.  \n",
       "3. With a 40‑minute stop:  \n",
       "   - Convert the stop time to hours: 40 min = 40/60 h = 2/3 h ≈ 0.6667 h.  \n",
       "   - Total elapsed time = 2 h + 2/3 h = 8/3 h ≈ 2.6667 h.  \n",
       "   - Average speed = 100 mi / (8/3 h) = 100 × 3/8 = 300/8 = 37.5 mph.\n",
       "\n",
       "**Answer**\n",
       "\n",
       "- Average speed without stopping: **50 mph**.  \n",
       "- Average speed when the car stops for 40 minutes: **37.5 mph**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = pick_model('groq', 'reason')\n",
    "llm = LLMClient('groq', model)\n",
    "\n",
    "problem = \"\"\"\n",
    "A car travels 100 miles in 2 hours. What is the average speed of the car?\n",
    "also if car stops for 40 minutes what is the average speed of the car?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_text, spec = render(\n",
    "            \"cot_reasoning.v1\",\n",
    "            role=\"math_tutor\",\n",
    "            instruction=\"\"\"slove the following problem step by step. \n",
    "            1. First identify whether car travelled the entire time without stopping or not.\n",
    "            2. If car stopped for x minutes and overall travelled for y the travel duration is y-x. So the speed should be d / (y-x).\n",
    "            3. If stopping time x mentioned do not add it to the travel duration. because it's already included in the travel duration of 2 hours.\n",
    "            So actual trael time is y (total travel time) - x (stopping time)\n",
    "            4. If car travelled the entire time without stopping then the average speed is d / y.\n",
    "            5. If car stopped for x minutes then the average speed is d / (y - x) \n",
    "            \"\"\",\n",
    "            problem=problem\n",
    "            )\n",
    "\n",
    "messages= [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{prompt_text}\"\n",
    "    }]\n",
    "\n",
    "response = llm.chat(messages, temperature=0.0)\n",
    "display(Markdown(response['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afa220",
   "metadata": {},
   "source": [
    "## 04 - COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d302c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-engineering-essentials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
