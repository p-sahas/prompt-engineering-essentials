{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 01: Decoding and Tokens\n",
        "\n",
        "**Objectives:**\n",
        "- Understand how text becomes tokens\n",
        "- Explore input/context/output token mapping\n",
        "- Experiment with decoding parameters (temperature, top_p, max_tokens)\n",
        "- Observe token count impact on cost and latency\n",
        "\n",
        "**Key Concepts:**\n",
        "- Tokenization is subword-based (BPE)\n",
        "- Different models use different encodings\n",
        "- Tokens ≠ Words (can be characters, subwords, or whole words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from utils.token_utils import pick_encoding, count_text_tokens\n",
        "from utils.logging_utils import log_llm_call\n",
        "from utils.llm_client import LLMClient\n",
        "from utils.router import pick_model\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Tokenization Basics\n",
        "\n",
        "Let's see how different text gets tokenized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text → Tokens\n",
            "================================================================================\n",
            "\n",
            "Text: Hello, world!\n",
            "Token count: 4\n",
            "Tokens: ['Hello', ',', ' world', '!']\n",
            "Token IDs: [13225, 11, 2375, 0]\n",
            "\n",
            "Text: The quick brown fox jumps over the lazy dog.\n",
            "Token count: 10\n",
            "Tokens: ['The', ' quick', ' brown', ' fox', ' jumps', ' over', ' the', ' lazy', ' dog', '.']\n",
            "Token IDs: [976, 4853, 19705, 68347, 65613, 1072, 290, 29082, 6446, 13]\n",
            "\n",
            "Text: Supercalifragilisticexpialidocious\n",
            "Token count: 10\n",
            "Tokens: ['Super', 'cal', 'if', 'rag', 'il', 'istic', 'exp', 'ial', 'id', 'ocious']\n",
            "Token IDs: [17260, 5842, 366, 17764, 311, 6207, 8067, 563, 315, 170661]\n",
            "\n",
            "Text: 你好世界\n",
            "Token count: 2\n",
            "Tokens: ['你好', '世界']\n",
            "Token IDs: [177519, 28428]\n",
            "\n",
            "Text: مرحبا بالعالم\n",
            "Token count: 4\n",
            "Tokens: ['مرح', 'با', ' بالع', 'الم']\n",
            "Token IDs: [158894, 26537, 101462, 12773]\n",
            "\n",
            "Text: print('Hello, World!')\n",
            "Token count: 7\n",
            "Tokens: ['print', \"('\", 'Hello', ',', ' World', '!', \"')\"]\n",
            "Token IDs: [1598, 706, 13225, 11, 5922, 0, 1542]\n"
          ]
        }
      ],
      "source": [
        "# Get encoding for OpenAI\n",
        "encoding = pick_encoding(\"openai\", \"gpt-4o\")\n",
        "\n",
        "# Example texts\n",
        "examples = [\n",
        "    \"Hello, world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Supercalifragilisticexpialidocious\",\n",
        "    \"你好世界\",  # Chinese\n",
        "    \"مرحبا بالعالم\",  # Arabic\n",
        "    \"print('Hello, World!')\",  # Code\n",
        "]\n",
        "\n",
        "print(\"Text → Tokens\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for text in examples:\n",
        "    tokens = encoding.encode(text)\n",
        "    decoded_tokens = [encoding.decode([t]) for t in tokens]\n",
        "    \n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(f\"Tokens: {decoded_tokens}\")\n",
        "    print(f\"Token IDs: {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Input vs Context vs Output Tokens\n",
        "\n",
        "Understanding the token breakdown:\n",
        "- **Input tokens**: Your prompt (system + user messages)\n",
        "- **Context tokens**: Additional context (e.g., RAG documents)\n",
        "- **Output tokens**: Model's response\n",
        "\n",
        "Cost and latency depend on total tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Breakdown:\n",
            "==================================================\n",
            "Input tokens (prompt):         26\n",
            "Context tokens:                40\n",
            "Estimated total (input):       69\n",
            "\n",
            "Note: Output tokens determined by model response length\n"
          ]
        }
      ],
      "source": [
        "from utils.token_utils import count_messages_tokens\n",
        "\n",
        "# Example with context\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful document summarizer.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Summarize the key points from the context.\"}\n",
        "]\n",
        "\n",
        "context_strs = [\n",
        "    \"CloudSync Pro is a file synchronization platform with real-time updates.\",\n",
        "    \"It supports AES-256 encryption and is GDPR compliant.\",\n",
        "    \"The service offers 99.9% uptime SLA and unlimited storage for enterprise.\"\n",
        "]\n",
        "\n",
        "# Count tokens separately\n",
        "token_breakdown = count_messages_tokens(messages, \"openai\", \"gpt-4o\", context_strs)\n",
        "\n",
        "print(\"Token Breakdown:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Input tokens (prompt):     {token_breakdown['input_tokens']:>6}\")\n",
        "print(f\"Context tokens:            {token_breakdown['context_tokens']:>6}\")\n",
        "print(f\"Estimated total (input):   {token_breakdown['estimated_total']:>6}\")\n",
        "print(f\"\\nNote: Output tokens determined by model response length\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Decoding Parameter Experiments\n",
        "\n",
        "Key parameters that affect output:\n",
        "- **temperature** (0.0-2.0): Controls randomness (0 = deterministic, higher = creative)\n",
        "- **max_tokens**: Limits output length\n",
        "- **top_p**: Nucleus sampling threshold\n",
        "\n",
        "Let's see how these affect the response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature Sweep (same prompt, different temperatures)\n",
            "================================================================================\n",
            "\n",
            "Temperature = 0.0\n",
            "Output: The crimson sun bled across the methane swamps of Xylos, painting the skeletal remains of the crashed starship in hues of rust and despair.\n",
            "\n",
            "Completion tokens: 30\n",
            "\n",
            "Temperature = 0.5\n",
            "Output: The crimson sun bled across the corrugated iron horizon of Neo-Dustbowl, painting the rust-colored shacks in hues of dying hope.\n",
            "\n",
            "Completion tokens: 29\n",
            "\n",
            "Temperature = 1.0\n",
            "Output: The bioluminescent moss pulsed with an eerie green light, casting long, skeletal shadows across the rusting hull of the abandoned starship, a silent testament to a war humanity had long forgotten.\n",
            "\n",
            "Completion tokens: 38\n",
            "\n",
            "Temperature = 1.5\n",
            "Output: The crimson sun bled across the metallic horizon, painting the chrome city of Veridium in hues of rust and regret.\n",
            "\n",
            "Completion tokens: 24\n"
          ]
        }
      ],
      "source": [
        "# Setup client\n",
        "model = pick_model(\"google\", \"general\")\n",
        "client = LLMClient(\"google\", model)\n",
        "\n",
        "prompt_msg = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write the opening sentence of a sci-fi story.\"}\n",
        "]\n",
        "\n",
        "# Temperature sweep\n",
        "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
        "\n",
        "print(\"Temperature Sweep (same prompt, different temperatures)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for temp in temperatures:\n",
        "    response = client.chat(prompt_msg, temperature=temp, max_tokens=50)\n",
        "    \n",
        "    print(f\"\\nTemperature = {temp}\")\n",
        "    print(f\"Output: {response['text']}\")\n",
        "    print(f\"Completion tokens: {response['usage']['completion_tokens_actual']}\")\n",
        "    \n",
        "    # Log it\n",
        "    log_llm_call(\n",
        "        provider=\"openai\",\n",
        "        model=model,\n",
        "        technique=f\"temp_sweep_{temp}\",\n",
        "        latency_ms=response['latency_ms'],\n",
        "        usage=response['usage'],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Max Tokens Limiting\n",
        "\n",
        "Control output length to manage cost and focus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Tokens Limiting\n",
            "================================================================================\n",
            "\n",
            "Max tokens = 10\n",
            "Output (10 tokens):\n",
            "Okay, imagine you have a light switch. A\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Max tokens = 30\n",
            "Output (30 tokens):\n",
            "Imagine a regular computer bit like a light switch: it can be either on (1) or off (0).  Quantum computers use **qubits\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Max tokens = 100\n",
            "Output (100 tokens):\n",
            "Okay, imagine regular computers use light switches: they can be either **on (1)** or **off (0)**.  These are called **bits**.\n",
            "\n",
            "Quantum computers are different.  They use **qubits**. Think of a dimmer switch instead of a light switch.  A qubit can be:\n",
            "\n",
            "* **On (1)**,\n",
            "* **Off (0)**,\n",
            "* **Or, and this is the key, *both on AND off at the same time***!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "max_tokens_values = [10, 30, 100]\n",
        "\n",
        "prompt_msg = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
        "]\n",
        "\n",
        "print(\"Max Tokens Limiting\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for max_tok in max_tokens_values:\n",
        "    response = client.chat(prompt_msg, temperature=0.7, max_tokens=max_tok)\n",
        "    \n",
        "    print(f\"\\nMax tokens = {max_tok}\")\n",
        "    print(f\"Output ({response['usage']['completion_tokens_actual']} tokens):\")\n",
        "    print(f\"{response['text']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Tokenization is subword-based**: One word can be multiple tokens, especially for rare words or non-English text\n",
        "2. **Input vs Output tokens**: Input tokens (your prompt) + output tokens (response) = total cost\n",
        "3. **Temperature controls randomness**: Use 0.0 for deterministic, 0.7-1.0 for creative tasks\n",
        "4. **max_tokens limits length**: Prevents runaway costs and focuses responses\n",
        "\n",
        "**Next:** `02_prompt_structure_patterns.ipynb` explores structured prompt engineering patterns.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
