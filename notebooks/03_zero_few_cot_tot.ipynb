{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Zero-Shot, Few-Shot, CoT, ToT\n",
        "\n",
        "**Objectives:**\n",
        "- Understand shot-based learning (zero, few)\n",
        "- Apply Chain-of-Thought (CoT) reasoning\n",
        "- Explore Tree-of-Thought (ToT) with branches\n",
        "- Automatic routing to reasoning models\n",
        "- Handle context overflow with overflow_summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from utils.prompts import render\n",
        "from utils.llm_client import LLMClient\n",
        "from utils.logging_utils import log_llm_call\n",
        "from utils.router import pick_model, should_use_reasoning_model\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Zero-Shot vs Few-Shot\n",
        "\n",
        "Zero-shot: No examples. Few-shot: Provide examples to guide behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot result: positive\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot classification\n",
        "prompt_text, spec = render(\n",
        "    'zero_shot.v1',\n",
        "    role='sentiment classifier',\n",
        "    instruction='Classify the sentiment as positive, negative, or neutral',\n",
        "    constraints='Return only the label',\n",
        "    format='Single word'\n",
        ")\n",
        "\n",
        "model = pick_model('openai', 'general')\n",
        "client = LLMClient('openai', model)\n",
        "\n",
        "test_text = 'This product is amazing! Best purchase ever.'\n",
        "messages = [{'role': 'user', 'content': f\"{prompt_text}\\n\\nText: {test_text}\"}]\n",
        "\n",
        "response = client.chat(messages, temperature=0.0)\n",
        "print('Zero-shot result:', response['text'])\n",
        "log_llm_call('openai', model, 'zero_shot', response['latency_ms'], response['usage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot result (nuanced):\n",
            "{'text': \"Sentiment: negative\\n\\nExplanation: User says the product seems nice, which is positive, but then states it's not for them, which is negative. Prioritizing the product quality statement, the sentiment is negative.\\n\", 'usage': {'input_tokens_est': 313, 'context_tokens_est': 0, 'total_est': 316, 'prompt_tokens_actual': 351, 'completion_tokens_actual': 45, 'total_tokens_actual': 396}, 'latency_ms': 796, 'raw': GenerateContentResponse(\n",
            "  automatic_function_calling_history=[],\n",
            "  candidates=[\n",
            "    Candidate(\n",
            "      avg_logprobs=-0.20678130255805122,\n",
            "      content=Content(\n",
            "        parts=[\n",
            "          Part(\n",
            "            text=\"\"\"Sentiment: negative\n",
            "\n",
            "Explanation: User says the product seems nice, which is positive, but then states it's not for them, which is negative. Prioritizing the product quality statement, the sentiment is negative.\n",
            "\"\"\"\n",
            "          ),\n",
            "        ],\n",
            "        role='model'\n",
            "      ),\n",
            "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
            "    ),\n",
            "  ],\n",
            "  model_version='gemini-2.0-flash-exp',\n",
            "  response_id='avdHabzDEYWqmtkPhN_biQU',\n",
            "  sdk_http_response=HttpResponse(\n",
            "    headers=<dict len=11>\n",
            "  ),\n",
            "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
            "    candidates_token_count=45,\n",
            "    candidates_tokens_details=[\n",
            "      ModalityTokenCount(\n",
            "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "        token_count=45\n",
            "      ),\n",
            "    ],\n",
            "    prompt_token_count=351,\n",
            "    prompt_tokens_details=[\n",
            "      ModalityTokenCount(\n",
            "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "        token_count=351\n",
            "      ),\n",
            "    ],\n",
            "    total_token_count=396\n",
            "  )\n",
            "), 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}\n"
          ]
        }
      ],
      "source": [
        "# Few-shot with comprehensive examples\n",
        "examples = \"\"\"\n",
        "Example 1:\n",
        "Review: I'm really happy with the product but it's bad!\n",
        "Sentiment: negative\n",
        "Explanation: User says product is bad but they are happy. When there's a conflict between \n",
        "user emotion and product quality, prioritize the product quality statement. \n",
        "Therefore, sentiment is negative.\n",
        "\n",
        "Example 2:\n",
        "Review: I'm really unhappy with the product but it's amazing!\n",
        "Sentiment: positive\n",
        "Explanation: User says product is amazing but they are unhappy. Prioritizing product \n",
        "quality over user emotion, the sentiment is positive.\n",
        "\n",
        "Example 3:\n",
        "Review: I'm really happy with the product! It's amazing!\n",
        "Sentiment: positive\n",
        "Explanation: Both user emotion and product quality are positive. Clear positive sentiment.\n",
        "\n",
        "Example 4:\n",
        "Review: I'm really unhappy with the product! It's terrible!\n",
        "Sentiment: negative\n",
        "Explanation: Both user emotion and product quality are negative. Clear negative sentiment.\n",
        "\n",
        "Example 5:\n",
        "Review: The product is okay, but it's not great.\n",
        "Sentiment: neutral\n",
        "Explanation: User expresses ambivalence. Product quality is neither strongly positive nor negative.\n",
        "\n",
        "Example 6:\n",
        "Review: I'm not sure about the product yet.\n",
        "Sentiment: neutral\n",
        "Explanation: User is uncertain and hasn't formed a clear opinion about product quality.\n",
        "\"\"\"\n",
        "\n",
        "# Use a more nuanced test case\n",
        "test_text_nuanced = \"It seems nice but it's not for me\"\n",
        "\n",
        "prompt_text, spec = render(\n",
        "    'few_shot.v1',\n",
        "    role='sentiment classifier',\n",
        "    examples=examples,\n",
        "    query=f'Review: {test_text_nuanced}',\n",
        "    constraints='Follow the pattern in examples: provide sentiment and brief explanation',\n",
        "    format='Sentiment: {{sentiment}}\\n\\nExplanation: {{explanation}}'\n",
        ")\n",
        "\n",
        "messages = [{'role': 'user', 'content': prompt_text}]\n",
        "response = client.chat(messages, temperature=0.2)\n",
        "print('Few-shot result (nuanced):')\n",
        "print(response)\n",
        "log_llm_call('openai', model, 'few_shot', response['latency_ms'], response['usage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using reasoning model: gemini-3-pro-preview\n",
            "CoT Response (Travel Time with Break):\n",
            "================================================================================\n",
            "{'text': '**Reasoning Steps:**\\n\\n1.  **Question 1:** Calculate the overall average speed using the total distance (100 miles) and the total time elapsed (2 hours). Formula: $\\\\text{Speed} = \\\\frac{\\\\text{Total Distance}}{\\\\text{Total Time}}$.\\n2.  **Question 2:** Determine the actual driving time. Since the 2-hour total includes the stop, subtract the stopping time from the total time.\\n    *   Total time = 120 minutes (2 hours).\\n    *   Stopping time = 40 minutes.\\n    *   Actual driving time = $120 - 40 = 80$ minutes.\\n3.  Convert the actual driving time into hours to calculate miles per hour (mph).\\n    *   $80 \\\\text{ minutes} = \\\\frac{80}{60} \\\\text{ hours} = \\\\frac{4}{3} \\\\text{ hours}$ (approx 1.33 hours).\\n4.  Calculate the average speed during driving time using $\\\\text{Speed} = \\\\frac{\\\\text{Total Distance}}{\\\\text{Actual Driving Time}}$.\\n\\n**Answer:**\\n\\n**Question 1:** 50 mph\\n**Question 2:** 75 mph', 'usage': {'input_tokens_est': 246, 'context_tokens_est': 0, 'total_est': 249, 'prompt_tokens_actual': 265, 'completion_tokens_actual': 269, 'total_tokens_actual': 534}, 'latency_ms': 13967, 'raw': GenerateContentResponse(\n",
            "  automatic_function_calling_history=[],\n",
            "  candidates=[\n",
            "    Candidate(\n",
            "      content=Content(\n",
            "        parts=[\n",
            "          Part(\n",
            "            text=\"\"\"**Reasoning Steps:**\n",
            "\n",
            "1.  **Question 1:** Calculate the overall average speed using the total distance (100 miles) and the total time elapsed (2 hours). Formula: $\\text{Speed} = \\frac{\\text{Total Distance}}{\\text{Total Time}}$.\n",
            "2.  **Question 2:** Determine the actual driving time. Since the 2-hour total includes the stop, subtract the stopping time from the total time.\n",
            "    *   Total time = 120 minutes (2 hours).\n",
            "    *   Stopping time = 40 minutes.\n",
            "    *   Actual driving time = $120 - 40 = 80$ minutes.\n",
            "3.  Convert the actual driving time into hours to calculate miles per hour (mph).\n",
            "    *   $80 \\text{ minutes} = \\frac{80}{60} \\text{ hours} = \\frac{4}{3} \\text{ hours}$ (approx 1.33 hours).\n",
            "4.  Calculate the average speed during driving time using $\\text{Speed} = \\frac{\\text{Total Distance}}{\\text{Actual Driving Time}}$.\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "**Question 1:** 50 mph\n",
            "**Question 2:** 75 mph\"\"\",\n",
            "            thought_signature=b'\\x12\\x84\\x1b\\n\\x81\\x1b\\x01r\\xc8\\xda|\\xa5\\xb0\\xa6_\\xaf\\x17*\\xddB\\xbf\\x1c\\x8c\\xf0\\x9c*\\xbd\\xc9\\xe9\\xeck_S\\xa7$\\xeb\\x10\\x1f\\x86,\\x9a\\xfddI\\xd5\\x8bG\\xeby\\xcf\\xdaj\\x01\\xf4?\\x999{\\x0e\\xd3\\r\\xb9\\x9cf/\\xfdS\\xfa\\x84\\xad\\xc3\\xeb/M\\xa5b\\x85\\x04\\xa44\\x89\\xe9W\\xe7\\xf8\\x99p\\xb5qf\\xe4\\x17\\xa8;M\\xa8\\xee...'\n",
            "          ),\n",
            "        ],\n",
            "        role='model'\n",
            "      ),\n",
            "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
            "      index=0\n",
            "    ),\n",
            "  ],\n",
            "  model_version='gemini-3-pro-preview',\n",
            "  response_id='ePdHac_aMNq8qtsPwL22sA0',\n",
            "  sdk_http_response=HttpResponse(\n",
            "    headers=<dict len=11>\n",
            "  ),\n",
            "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
            "    candidates_token_count=269,\n",
            "    prompt_token_count=265,\n",
            "    prompt_tokens_details=[\n",
            "      ModalityTokenCount(\n",
            "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "        token_count=265\n",
            "      ),\n",
            "    ],\n",
            "    thoughts_token_count=1044,\n",
            "    total_token_count=1578\n",
            "  )\n",
            "), 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# CoT auto-routes to reasoning model\n",
        "reasoning_model = pick_model('google', 'cot')\n",
        "print(f'Using reasoning model: {reasoning_model}')\n",
        "\n",
        "client_reasoning = LLMClient('google', reasoning_model)\n",
        "\n",
        "# Problem from live class: break time vs travel time confusion\n",
        "problem = \"\"\"A car travels 100 miles in 2 hours. \n",
        "\n",
        "Question 1: What is the average speed of the car?\n",
        "Question 2: If the car stopped for 40 minutes during this 2-hour journey, \n",
        "what was the average speed during the actual driving time?\n",
        "\n",
        "Important: The 2 hours already includes the 40-minute stop.\"\"\"\n",
        "\n",
        "# Additional guidance\n",
        "instruction = \"\"\"Solve the following problem step by step.\n",
        "1. First identify whether the car travelled the entire time without stopping or not.\n",
        "2. If car stopped for x minutes and overall travelled for y hours, the actual driving duration is y-x.\n",
        "3. If stopping time x is mentioned, do not add it to the travel duration because it's already included.\n",
        "   So actual travel time is y (total time) - x (stopping time).\n",
        "4. If car travelled the entire time without stopping, then average speed is distance / y.\n",
        "5. If car stopped for x minutes, then average speed during driving is distance / (y - x).\"\"\"\n",
        "\n",
        "prompt_text, spec = render(\n",
        "    'cot_reasoning.v1',\n",
        "    role='math tutor',\n",
        "    problem=problem\n",
        ")\n",
        "\n",
        "# Combine problem with instruction (as done in live class)\n",
        "full_prompt = f\"\"\"text: {prompt_text}\n",
        "\n",
        "instruction: {instruction}\"\"\"\n",
        "\n",
        "# FIX: Remove space before 'role' to avoid KeyError for Google provider\n",
        "messages = [{'role': 'user', 'content': full_prompt}]\n",
        "response = client_reasoning.chat(messages, temperature=spec.temperature, max_tokens=spec.max_tokens)\n",
        "\n",
        "print('CoT Response (Travel Time with Break):')\n",
        "print('=' * 80)\n",
        "print(response)\n",
        "print('=' * 80)\n",
        "log_llm_call('openai', reasoning_model, 'cot', response['latency_ms'], response['usage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using reasoning model: gemini-3-pro-preview\n",
            "CoT Response (Travel Time with Break):\n",
            "================================================================================\n",
            "{'text': '**Reasoning Steps:**\\n\\n1.  **Analyze Question 1 (Overall Average Speed):**\\n    *   Identify total distance ($d$) and total time ($y$).\\n    *   $d = 100$ miles, $y = 2$ hours.\\n    *   Apply formula: $\\\\text{Average Speed} = \\\\text{Total Distance} / \\\\text{Total Time}$.\\n\\n2.  **Analyze Question 2 (Actual Driving Speed):**\\n    *   Identify the stopping time ($x$) included in the total time. $x = 40$ minutes.\\n    *   Convert stopping time to hours: $40 \\\\text{ minutes} = \\\\frac{40}{60} \\\\text{ hours} = \\\\frac{2}{3} \\\\text{ hours}$.\\n    *   Calculate actual driving time ($t_{driving}$) by subtracting stopping time from total time ($y - x$): $2 \\\\text{ hours} - \\\\frac{2}{3} \\\\text{ hours} = \\\\frac{4}{3} \\\\text{ hours}$ (or 1 hour 20 minutes).\\n    *   Apply formula: $\\\\text{Driving Speed} = \\\\text{Total Distance} / \\\\text{Actual Driving Time}$.\\n    *   Calculation: $100 / (\\\\frac{4}{3})$.\\n\\n**Answer:**\\n\\n**Question 1:** 50 mph\\n**Question 2:** 75 mph', 'usage': {'input_tokens_est': 246, 'context_tokens_est': 0, 'total_est': 249, 'prompt_tokens_actual': 265, 'completion_tokens_actual': 311, 'total_tokens_actual': 576}, 'latency_ms': 13651, 'raw': GenerateContentResponse(\n",
            "  automatic_function_calling_history=[],\n",
            "  candidates=[\n",
            "    Candidate(\n",
            "      content=Content(\n",
            "        parts=[\n",
            "          Part(\n",
            "            text=\"\"\"**Reasoning Steps:**\n",
            "\n",
            "1.  **Analyze Question 1 (Overall Average Speed):**\n",
            "    *   Identify total distance ($d$) and total time ($y$).\n",
            "    *   $d = 100$ miles, $y = 2$ hours.\n",
            "    *   Apply formula: $\\text{Average Speed} = \\text{Total Distance} / \\text{Total Time}$.\n",
            "\n",
            "2.  **Analyze Question 2 (Actual Driving Speed):**\n",
            "    *   Identify the stopping time ($x$) included in the total time. $x = 40$ minutes.\n",
            "    *   Convert stopping time to hours: $40 \\text{ minutes} = \\frac{40}{60} \\text{ hours} = \\frac{2}{3} \\text{ hours}$.\n",
            "    *   Calculate actual driving time ($t_{driving}$) by subtracting stopping time from total time ($y - x$): $2 \\text{ hours} - \\frac{2}{3} \\text{ hours} = \\frac{4}{3} \\text{ hours}$ (or 1 hour 20 minutes).\n",
            "    *   Apply formula: $\\text{Driving Speed} = \\text{Total Distance} / \\text{Actual Driving Time}$.\n",
            "    *   Calculation: $100 / (\\frac{4}{3})$.\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "**Question 1:** 50 mph\n",
            "**Question 2:** 75 mph\"\"\",\n",
            "            thought_signature=b'\\x12\\x85\\x1a\\n\\x82\\x1a\\x01r\\xc8\\xda|-^T\\\\V\\xd2C\\x8aU\\xcf(:6\\x97#H\\xda\\x9d$\\xde\\xb8\\r\\x00O>\\xc8\\x0f\\x8e)\\xb6\\x1e\\xe3\\xe6\\x97X\\x05\\x06\\xa0\\xfe\\xd9\\xfb\\xfa}l\\x84$]7\\xd6\\x8c\\x14pJ\\x196\\x89\\xb0\\xed\\x0f\\xb9\\x89!\\xc3\\x978/\\xedv4b\\xc5\\xcfd\\xc0P\\xfe\\xd3(\\x1f\\x0cTaN\\xd5\\x94\\xb7...'\n",
            "          ),\n",
            "        ],\n",
            "        role='model'\n",
            "      ),\n",
            "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
            "      index=0\n",
            "    ),\n",
            "  ],\n",
            "  model_version='gemini-3-pro-preview',\n",
            "  response_id='hvdHabm9IJqIqtsPovH8sA0',\n",
            "  sdk_http_response=HttpResponse(\n",
            "    headers=<dict len=11>\n",
            "  ),\n",
            "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
            "    candidates_token_count=311,\n",
            "    prompt_token_count=265,\n",
            "    prompt_tokens_details=[\n",
            "      ModalityTokenCount(\n",
            "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "        token_count=265\n",
            "      ),\n",
            "    ],\n",
            "    thoughts_token_count=991,\n",
            "    total_token_count=1567\n",
            "  )\n",
            "), 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# CoT auto-routes to reasoning model\n",
        "reasoning_model = pick_model('google', 'cot')\n",
        "print(f'Using reasoning model: {reasoning_model}')\n",
        "\n",
        "client_reasoning = LLMClient('google', reasoning_model)\n",
        "\n",
        "# Problem from live class: break time vs travel time confusion\n",
        "problem = \"\"\"A car travels 100 miles in 2 hours. \n",
        "\n",
        "Question 1: What is the average speed of the car?\n",
        "Question 2: If the car stopped for 40 minutes during this 2-hour journey, \n",
        "what was the average speed during the actual driving time?\n",
        "\n",
        "Important: The 2 hours already includes the 40-minute stop.\"\"\"\n",
        "\n",
        "# Additional guidance (from live class approach)\n",
        "instruction = \"\"\"Solve the following problem step by step.\n",
        "1. First identify whether the car travelled the entire time without stopping or not.\n",
        "2. If car stopped for x minutes and overall travelled for y hours, the actual driving duration is y-x.\n",
        "3. If stopping time x is mentioned, do not add it to the travel duration because it's already included.\n",
        "   So actual travel time is y (total time) - x (stopping time).\n",
        "4. If car travelled the entire time without stopping, then average speed is distance / y.\n",
        "5. If car stopped for x minutes, then average speed during driving is distance / (y - x).\"\"\"\n",
        "\n",
        "prompt_text, spec = render(\n",
        "    'cot_reasoning.v1',\n",
        "    role='math tutor',\n",
        "    problem=problem\n",
        ")\n",
        "\n",
        "# Combine problem with instruction (as done in live class)\n",
        "full_prompt = f\"\"\"text: {prompt_text}\n",
        "\n",
        "instruction: {instruction}\"\"\"\n",
        "\n",
        "messages = [{'role': 'user', 'content': full_prompt}]\n",
        "response = client_reasoning.chat(messages, temperature=spec.temperature, max_tokens=spec.max_tokens)\n",
        "\n",
        "print('CoT Response (Travel Time with Break):')\n",
        "print('=' * 80)\n",
        "print(response)\n",
        "print('=' * 80)\n",
        "log_llm_call('google', reasoning_model, 'cot', response['latency_ms'], response['usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Tree-of-Thought (ToT)\n",
        "\n",
        "Explore multiple solution paths, then select the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToT Response (Multiple Solution Paths):\n",
            "================================================================================\n",
            "{'text': None, 'usage': {'input_tokens_est': 76, 'context_tokens_est': 0, 'total_est': 79, 'prompt_tokens_actual': 76, 'completion_tokens_actual': 0, 'total_tokens_actual': 76}, 'latency_ms': 43435, 'raw': GenerateContentResponse(\n",
            "  automatic_function_calling_history=[],\n",
            "  candidates=[\n",
            "    Candidate(\n",
            "      content=Content(),\n",
            "      finish_reason=<FinishReason.MAX_TOKENS: 'MAX_TOKENS'>,\n",
            "      index=0\n",
            "    ),\n",
            "  ],\n",
            "  model_version='gemini-3-pro-preview',\n",
            "  response_id='sfdHafbmO8mQmtkP24-AGQ',\n",
            "  sdk_http_response=HttpResponse(\n",
            "    headers=<dict len=11>\n",
            "  ),\n",
            "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
            "    prompt_token_count=76,\n",
            "    prompt_tokens_details=[\n",
            "      ModalityTokenCount(\n",
            "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "        token_count=76\n",
            "      ),\n",
            "    ],\n",
            "    thoughts_token_count=4093,\n",
            "    total_token_count=4169\n",
            "  )\n",
            "), 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "problem = \"\"\"You have a 3-gallon jug and a 5-gallon jug.\n",
        "How can you measure exactly 4 gallons?\"\"\"\n",
        "\n",
        "prompt_text, spec = render(\n",
        "    'tot_reasoning.v1',\n",
        "    role='puzzle solver',\n",
        "    problem=problem,\n",
        "    branches='3'\n",
        ")\n",
        "\n",
        "messages = [{'role': 'user', 'content': prompt_text}]\n",
        "response = client_reasoning.chat(messages, temperature=spec.temperature, max_tokens=spec.max_tokens)\n",
        "\n",
        "print('ToT Response (Multiple Solution Paths):')\n",
        "print('=' * 80)\n",
        "print(response)\n",
        "print('=' * 80)\n",
        "log_llm_call('openai', reasoning_model, 'tot', response['latency_ms'], response['usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Context Overflow Demo\n",
        "\n",
        "When context exceeds limits, use overflow_summarize prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overflow handled: True\n",
            "Response: The provided context consists of a product description with a long list of feature descriptions. Due to the truncation, the specific features, names, and numbers are unavailable. The summary is theref ...\n"
          ]
        }
      ],
      "source": [
        "# Simulate large context\n",
        "large_context = 'Product details: ' + ' '.join(['Feature description.'] * 500)\n",
        "\n",
        "prompt_text, spec = render(\n",
        "    'overflow_summarize.v1',\n",
        "    context=large_context,\n",
        "    max_tokens_context='200',\n",
        "    task='List the top 3 features',\n",
        "    format='Bullet list'\n",
        ")\n",
        "\n",
        "# Use hard_prompt_cap to trigger truncation\n",
        "client_capped = LLMClient('google', model, hard_prompt_cap=300)\n",
        "messages = [{'role': 'user', 'content': prompt_text}]\n",
        "\n",
        "response = client_capped.chat(messages, temperature=0.2)\n",
        "print('Overflow handled:', response['meta']['overflow_handled'])\n",
        "print('Response:', response['text'][:200], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Zero-shot**: Fast, works for simple tasks\n",
        "2. **Few-shot**: Examples improve accuracy significantly\n",
        "3. **CoT/ToT**: Reasoning models required, higher token cost but better logic\n",
        "4. **Automatic routing**: `pick_model()` selects reasoning models for CoT/ToT\n",
        "5. **Overflow handling**: Use summarization or truncation when context is too large\n",
        "\n",
        "**Next:** `04_structured_outputs_json_schema.ipynb`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prompt-engineering-essentials",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
