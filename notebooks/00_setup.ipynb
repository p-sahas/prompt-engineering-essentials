{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 00: Setup & Verification\n",
        "\n",
        "**Objectives:**\n",
        "- Verify API keys for all providers\n",
        "- Test basic connectivity with OpenAI, Google Gemini, and Groq\n",
        "- Understand token estimation vs actual token usage\n",
        "- Initialize logging to `logs/runs.csv`\n",
        "\n",
        "**Prerequisites:**\n",
        "- Copy `.env.sample` to `.env` and add your API keys\n",
        "- Install dependencies: `pip install -r requirements.txt` or `uv sync`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key Status:\n",
            "--------------------------------------------------\n",
            "OpenAI               ✓ Found    (...esp_VQoA)\n",
            "Google Gemini        ✓ Found    (...ukLZ55NY)\n",
            "Groq                 ✓ Found    (...L0uHiohh)\n"
          ]
        }
      ],
      "source": [
        "# Setup imports and environment\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys\n",
        "providers = {\n",
        "            \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "            \"Google Gemini\": os.getenv(\"GEMINI_API_KEY\"),\n",
        "            \"Groq\": os.getenv(\"GROQ_API_KEY\"),\n",
        "            }\n",
        "\n",
        "print(\"API Key Status:\")\n",
        "print(\"-\" * 50)\n",
        "for provider, key in providers.items():\n",
        "    status = \"✓ Found\" if key else \"✗ Missing\"\n",
        "    preview = f\"(...{key[-8:]})\" if key else \"\"\n",
        "    print(f\"{provider:20s} {status:10s} {preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Utilities\n",
        "\n",
        "Import our custom utilities for token counting, logging, and LLM client abstraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available prompts: 11\n",
            "Prompts: skeleton.v1, zero_shot.v1, few_shot.v1, cot_reasoning.v1, tot_reasoning.v1, json_extract.v1, tool_call.v1, overflow_summarize.v1, rate_limit_retry.v1, style_persona.v1, router_classify.v1\n"
          ]
        }
      ],
      "source": [
        "from utils.prompts import render, PROMPTS, list_prompts\n",
        "from utils.llm_client import LLMClient\n",
        "from utils.logging_utils import log_llm_call, get_log_summary\n",
        "from utils.router import pick_model\n",
        "from utils.token_utils import count_messages_tokens, reconcile_usage\n",
        "\n",
        "print(f\"Available prompts: {len(list_prompts())}\")\n",
        "print(f\"Prompts: {', '.join(list_prompts())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Hello World with Token Tracking\n",
        "\n",
        "Let's send a simple hello-world request to each provider and compare:\n",
        "- **Estimated tokens** (via tiktoken)\n",
        "- **Actual tokens** (from provider API)\n",
        "\n",
        "This demonstrates that tiktoken provides good estimates for OpenAI, but approximate counts for other providers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Testing: openai\n",
            "============================================================\n",
            "Model: gpt-4o-mini\n",
            "\n",
            "Response: Hello, World!\n",
            "Latency: 6240 ms\n",
            "\n",
            "Token Usage:\n",
            "  Estimated Input:      24 tokens\n",
            "  Actual Prompt:        27 tokens\n",
            "  Actual Completion:     4 tokens\n",
            "  Total (actual):       31 tokens\n",
            "\n",
            "Estimation Accuracy: 88.9%\n",
            "\n",
            "============================================================\n",
            "Testing: google\n",
            "============================================================\n",
            "Model: gemini-2.0-flash-exp\n",
            "✗ Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\\nPlease retry in 38.75079881s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-exp', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-exp'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-exp'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '38s'}]}}\n",
            "\n",
            "============================================================\n",
            "Testing: groq\n",
            "============================================================\n",
            "Model: llama-3.1-8b-instant\n",
            "\n",
            "Response: Hello, World!\n",
            "Latency: 1347 ms\n",
            "\n",
            "Token Usage:\n",
            "  Estimated Input:      24 tokens\n",
            "  Actual Prompt:        51 tokens\n",
            "  Actual Completion:     5 tokens\n",
            "  Total (actual):       56 tokens\n",
            "\n",
            "Estimation Accuracy: 47.1%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def test_provider(provider_name):\n",
        "    \"\"\"Test a provider with a simple hello world.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {provider_name}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    # Pick appropriate model\n",
        "    model = pick_model(provider_name.lower(), \"general\")\n",
        "    print(f\"Model: {model}\")\n",
        "    \n",
        "    # Create client\n",
        "    client = LLMClient(provider=provider_name.lower(), model=model)\n",
        "    \n",
        "    # Simple message\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Say 'Hello, World!' and nothing else.\"}\n",
        "    ]\n",
        "    \n",
        "    # Call API\n",
        "    try:\n",
        "        response = client.chat(messages, max_tokens=20, temperature=0.0)\n",
        "        \n",
        "        print(f\"\\nResponse: {response['text']}\")\n",
        "        print(f\"Latency: {response['latency_ms']} ms\")\n",
        "        print(f\"\\nToken Usage:\")\n",
        "        print(f\"  Estimated Input:  {response['usage']['input_tokens_est']:>6} tokens\")\n",
        "        print(f\"  Actual Prompt:    {response['usage']['prompt_tokens_actual']:>6} tokens\")\n",
        "        print(f\"  Actual Completion:{response['usage']['completion_tokens_actual']:>6} tokens\")\n",
        "        print(f\"  Total (actual):   {response['usage']['total_tokens_actual']:>6} tokens\")\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        if response['usage']['prompt_tokens_actual']:\n",
        "            est = response['usage']['input_tokens_est']\n",
        "            act = response['usage']['prompt_tokens_actual']\n",
        "            accuracy = (1 - abs(est - act) / act) * 100\n",
        "            print(f\"\\nEstimation Accuracy: {accuracy:.1f}%\")\n",
        "        \n",
        "        # Log to CSV\n",
        "        log_llm_call(\n",
        "            provider=provider_name.lower(),\n",
        "            model=model,\n",
        "            technique=\"hello_world\",\n",
        "            latency_ms=response['latency_ms'],\n",
        "            usage=response['usage'],\n",
        "            retry_count=response['meta']['retry_count'],\n",
        "            backoff_ms_total=response['meta']['backoff_ms_total'],\n",
        "        )\n",
        "        \n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test all providers that have keys\n",
        "results = {}\n",
        "for provider, key in providers.items():\n",
        "    if key:\n",
        "        provider_name = provider.split()[0].lower()  # \"Google Gemini\" -> \"google\"\n",
        "        if provider_name == \"google\":\n",
        "            provider_name = \"google\"\n",
        "        results[provider] = test_provider(provider_name)\n",
        "    else:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Skipping {provider} (no API key)\")\n",
        "        print('='*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total LLM calls logged: 59\n",
            "\n",
            "Most recent calls:\n",
            "                     timestamp provider                 model    technique  latency_ms  input_tokens_est  context_tokens_est  total_est  prompt_tokens_actual  completion_tokens_actual  total_tokens_actual  retry_count  backoff_ms_total  overflow_handled cost_estimate_usd  notes\n",
            "56  2025-12-22T00:35:46.037908   openai  gemini-3-pro-preview          tot       43435                76                   0         79                    76                       NaN                   76            0                 0             False               NaN    NaN\n",
            "57  2025-12-29T17:11:12.788369   openai           gpt-4o-mini  hello_world        6240                24                   0         27                    27                       4.0                   31            0                 0             False        ~$0.000006    NaN\n",
            "58  2025-12-29T17:11:22.049771     groq  llama-3.1-8b-instant  hello_world        1347                24                   0         27                    51                       5.0                   56            0                 0             False        ~$0.000003    NaN\n",
            "\n",
            "============================================================\n",
            "Session Summary:\n",
            "============================================================\n",
            "total_calls: 59\n",
            "avg_latency_ms: 5083.322033898305\n",
            "total_retries: 0\n",
            "techniques_used:\n",
            "  cot: 17\n",
            "  zero_shot: 10\n",
            "  few_shot: 10\n",
            "  hello_world: 8\n",
            "  tot: 7\n",
            "  temp_sweep_1.0: 1\n",
            "  temp_sweep_0.0: 1\n",
            "  temp_sweep_0.5: 1\n",
            "  zero_shot_classify: 1\n",
            "  skeleton: 1\n",
            "  unstructured: 1\n",
            "  temp_sweep_1.5: 1\n",
            "providers_used:\n",
            "  openai: 43\n",
            "  google: 13\n",
            "  groq: 3\n",
            "total_prompt_tokens: 9942\n",
            "total_completion_tokens: 1331\n"
          ]
        }
      ],
      "source": [
        "# View recent logs\n",
        "try:\n",
        "    logs_df = pd.read_csv(\"../logs/runs.csv\")\n",
        "    print(f\"\\nTotal LLM calls logged: {len(logs_df)}\")\n",
        "    print(\"\\nMost recent calls:\")\n",
        "    print(logs_df.tail(3).to_string())\n",
        "    \n",
        "    # Summary statistics\n",
        "    summary = get_log_summary()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Session Summary:\")\n",
        "    print('='*60)\n",
        "    for key, value in summary.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"{key}:\")\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No logs found yet. Run some LLM calls first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Token Estimation**: tiktoken provides accurate estimates for OpenAI models, approximate for others\n",
        "2. **Provider Usage**: Actual token counts come from provider APIs when available\n",
        "3. **Logging**: Every call is logged to `logs/runs.csv` with full token metrics\n",
        "4. **Reconciliation**: We track both estimated and actual tokens for transparency\n",
        "\n",
        "**Next:** Proceed to `01_decoding_and_tokens.ipynb` to dive deeper into tokenization pedagogy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prompt-engineering-essentials",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
